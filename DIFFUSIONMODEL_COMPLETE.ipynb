{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyODrAA0yESMj5EZO5wrE3XE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azycr4yy/DDPM/blob/main/DIFFUSIONMODEL_COMPLETE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbICCcBnlduV"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"subinium/emojiimage-dataset\")\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"jessicali9530/celeba-dataset\")\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "CZeB0KGIxCl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "grXdAyFuApzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "hAm7vP8IBcTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d subinium/emojiimage-dataset"
      ],
      "metadata": {
        "id": "WPvk3X8gBfHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d jessicali9530/celeba-dataset"
      ],
      "metadata": {
        "id": "AMaTP5c2xN8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o emojiimage-dataset.zip -d emoji_dataset;"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zTvcislJB4Ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q celeba-dataset.zip -d img_align_celeba\n"
      ],
      "metadata": {
        "id": "MZmm8YCKxUAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(os.listdir(\"img_align_celeba\")[:5])\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GSfiCfVZCpr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('emoji_dataset/full_emoji.csv')\n",
        "df.head()\n",
        "df = df['name']"
      ],
      "metadata": {
        "id": "znWuajU_Cs9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "data = []\n",
        "for root, dirs, files in os.walk(\"img_align_celeba\"):\n",
        "    for file in files:\n",
        "        if file.endswith('.png') or file.endswith('.jpg'):\n",
        "            path = os.path.join(root, file)\n",
        "            data.append({'path': path})\n",
        "path_data_base = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "VBj02sfeDUhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "class EmojiDataset(Dataset):\n",
        "  def __init__(self,dataframe,transform):\n",
        "    self.dataframe=dataframe\n",
        "    self.transform=transform\n",
        "  def __len__(self):\n",
        "    return len(self.dataframe)\n",
        "  def __getitem__(self,idx):\n",
        "    img_path = self.dataframe.iloc[idx][\"path\"]\n",
        "    img = Image.open(img_path)\n",
        "    if img.mode == 'P':\n",
        "      img = img.convert('RGB')\n",
        "    else:\n",
        "      img = img.convert('RGB')\n",
        "    if self.transform:\n",
        "      img = self.transform(img)\n",
        "    return img"
      ],
      "metadata": {
        "id": "7_bnX6CaHDVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "LTnw57BKhQoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.CenterCrop(64),\n",
        "    transforms.Resize(64),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "data = EmojiDataset(path_data_base,transform)\n",
        "dataloader = DataLoader(data,batch_size=2,shuffle=True)"
      ],
      "metadata": {
        "id": "RaqJTuNTHRAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#display training batch images\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "images = next(iter(dataloader))\n",
        "images = images * 0.5 + 0.5\n",
        "fig, axes = plt.subplots(4, 8, figsize=(15, 7))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    if i < images.shape[0]:\n",
        "        ax.imshow(np.transpose(images[i].numpy(), (1, 2, 0)))\n",
        "        ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e8A9lbofH_uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "class NoiseScheduler():\n",
        "  def __init__(self,betas_start,betas_end,num_time_steps) -> None:\n",
        "    self.betas_start = betas_start\n",
        "    self.betas_end = betas_end\n",
        "    self.num_time_steps = num_time_steps\n",
        "    self.beta = torch.linspace(self.betas_start,self.betas_end,self.num_time_steps)\n",
        "    self.alpha = 1 - self.beta\n",
        "    self.alpha_bar = torch.cumprod(self.alpha,dim=0)\n",
        "    self.sqrt_alpha_bar = torch.sqrt(self.alpha_bar)\n",
        "    self.sqrt_one_minus_alpha_bar = torch.sqrt(1 - self.alpha_bar)\n",
        "  def add_noise(self,img,noise,t):\n",
        "    img_shape = img.shape\n",
        "    batch_size = img_shape[0]\n",
        "\n",
        "    sqrt_alpha_bar = self.sqrt_alpha_bar.to(img.device, dtype=img.dtype)[t].reshape(batch_size)\n",
        "    sqrt_one_minus_alpha_bar = self.sqrt_one_minus_alpha_bar.to(img.device, dtype=img.dtype)[t].reshape(batch_size)\n",
        "    for _ in range(len(img_shape)-1):\n",
        "      sqrt_alpha_bar = sqrt_alpha_bar.unsqueeze(-1)\n",
        "      sqrt_one_minus_alpha_bar = sqrt_one_minus_alpha_bar.unsqueeze(-1)\n",
        "    return sqrt_alpha_bar * img + sqrt_one_minus_alpha_bar * noise\n",
        "  def sample_prev_timestep(self,xt,noise_prod,t):\n",
        "    x0 = (xt-self.sqrt_one_minus_alpha_bar[t]*noise_prod)/self.sqrt_alpha_bar[t]\n",
        "    x0 = torch.clamp(x0,min=-1,max=1)\n",
        "    mean = xt-(self.beta[t]*noise_prod)/self.sqrt_one_minus_alpha_bar[t]\n",
        "    mean = mean/self.sqrt_alpha_bar[t]\n",
        "    if t==0:\n",
        "      return mean,x0\n",
        "    else:\n",
        "      variance = (self.alpha[t])*(self.sqrt_one_minus_alpha_bar[t-1])\n",
        "      variance = variance/self.sqrt_one_minus_alpha_bar[t]\n",
        "      sigma = variance **0.5\n",
        "      noise = torch.randn_like(xt).to(device)\n",
        "      return mean+noise*sigma,x0"
      ],
      "metadata": {
        "id": "-FF86a8gJzEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_time_emb(time_steps,time_emd_dim):\n",
        "  time_steps=time_steps.float()\n",
        "  half_dim = time_emd_dim // 2\n",
        "  freqs = torch.exp(-torch.arange(half_dim, device=time_steps.device, dtype=time_steps.dtype).float() *\n",
        "                      torch.log(torch.tensor(10000.0, device=time_steps.device, dtype=time_steps.dtype)) / (half_dim - 1))\n",
        "  args = time_steps[:, None] * freqs[None, :]\n",
        "  t_emb = torch.cat([torch.sin(args), torch.cos(args)], dim=1)\n",
        "  return t_emb"
      ],
      "metadata": {
        "id": "cehraFP9K7g-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DownBlock(nn.Module):\n",
        "  def __init__(self,in_channels,out_channels,num_heads,t_emb_dim,down_sample) -> None:\n",
        "    super().__init__()\n",
        "    self.down_sample = down_sample\n",
        "    self.first_conv_resnet = nn.Sequential(\n",
        "        nn.GroupNorm(8,in_channels),\n",
        "        nn.SiLU(),\n",
        "        nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1,stride=1)\n",
        "    )\n",
        "    self.t_emb_layer = nn.Sequential(\n",
        "        nn.SiLU(),\n",
        "        nn.Linear(t_emb_dim,out_channels)\n",
        "    )\n",
        "    self.second_conv_conv = nn.Sequential(\n",
        "        nn.GroupNorm(8,out_channels),\n",
        "        nn.SiLU(),\n",
        "        nn.Conv2d(out_channels,out_channels,kernel_size=3,padding=1,stride=1)\n",
        "    )\n",
        "    self.down_Sampler = nn.Conv2d(out_channels,out_channels,kernel_size=4,stride=2,padding=1)\n",
        "    self.attention_norm = nn.GroupNorm(8,out_channels)\n",
        "    self.MultiHeadAttentionm = nn.MultiheadAttention(num_heads=num_heads,embed_dim=out_channels)\n",
        "    self.residual_input_conv = nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=1)\n",
        "\n",
        "  def forward(self,x,t_emb):\n",
        "    out = x\n",
        "    resnet_input = x\n",
        "    out = self.first_conv_resnet(out)\n",
        "    out = out + self.t_emb_layer(t_emb)[:,:,None,None]\n",
        "    out = self.second_conv_conv(out)\n",
        "    out = out + self.residual_input_conv(resnet_input)\n",
        "\n",
        "    batch_size , channels , h , w = out.shape\n",
        "    in_attn = out\n",
        "    in_attn = in_attn.reshape(batch_size,channels,h*w)\n",
        "    in_attn = self.attention_norm(in_attn)\n",
        "    in_attn = in_attn.transpose(1,2)\n",
        "    out_attn , _ = self.MultiHeadAttentionm(in_attn,in_attn,in_attn)\n",
        "    out_attn = out_attn.transpose(1,2).reshape(batch_size,channels,h,w)\n",
        "    out = out_attn + out\n",
        "    if self.down_sample:\n",
        "      out = self.down_Sampler(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "fgi-xps3klHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MiddleBlock(nn.Module):\n",
        "  def __init__(self, in_channels,out_channels,t_emb_dim,num_heads) -> None:\n",
        "    super().__init__()\n",
        "    self.resenet_conv_first = nn.Sequential(\n",
        "        nn.GroupNorm(8,in_channels),\n",
        "        nn.SiLU(),\n",
        "        nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=3,padding=1,stride=1)\n",
        "    )\n",
        "    self.t_emb_layer = nn.Sequential(\n",
        "        nn.SiLU(),\n",
        "        nn.Linear(t_emb_dim,out_channels)\n",
        "    )\n",
        "    self.resenet_conv_second = nn.Sequential(\n",
        "        nn.GroupNorm(8,out_channels),\n",
        "        nn.SiLU(),\n",
        "        nn.Conv2d(in_channels=out_channels,out_channels=out_channels,kernel_size=3,padding=1,stride=1)\n",
        "    )\n",
        "    self.attention_norm = nn.GroupNorm(8,out_channels)\n",
        "    self.attentiion = nn.MultiheadAttention(out_channels,num_heads,batch_first=True)\n",
        "    self.residual_input = nn.ModuleList([\n",
        "        nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=1),\n",
        "        nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=1)\n",
        "    ])\n",
        "  def forward(self,x,t_emb):\n",
        "    out = x\n",
        "    resnet_input = out\n",
        "    out = self.resenet_conv_first(out)\n",
        "    out = out + self.t_emb_layer(t_emb)[:,:,None,None]\n",
        "    out = self.resenet_conv_second(out)\n",
        "    out = out + self.residual_input[0](resnet_input)\n",
        "\n",
        "    batch_size , channels , h , w = out.shape\n",
        "    in_attn = out.reshape(batch_size,channels,h*w)\n",
        "    in_attn = self.attention_norm(in_attn)\n",
        "    in_attn = in_attn.transpose(1,2)\n",
        "    out_attn , _ = self.attentiion(in_attn,in_attn,in_attn)\n",
        "    out_attn = out_attn.transpose(1,2).reshape(batch_size,channels,h,w)\n",
        "    out = out_attn + out\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "5odUHE95Ck_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResnetBlock(nn.Module):\n",
        "  def __init__(self,in_channels,out_channels,t_emb_dim,num_heads=4) -> None:\n",
        "    super().__init__()\n",
        "    self.norm1 = nn.GroupNorm(8,in_channels)\n",
        "    self.conv1 = nn.Conv2d(in_channels=in_channels,out_channels=out_channels,kernel_size=3,padding=1,stride=1)\n",
        "    self.norm2 = nn.GroupNorm(8,out_channels)\n",
        "    self.conv2 = nn.Conv2d(in_channels=out_channels,out_channels=out_channels,kernel_size=3,padding=1,stride=1)\n",
        "    self.t_emb_layer = nn.Sequential(\n",
        "        nn.SiLU(),\n",
        "        nn.Linear(t_emb_dim,out_channels)\n",
        "    )\n",
        "    if in_channels != out_channels:\n",
        "      self.residual_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "    else:\n",
        "      self.residual_conv = nn.Identity()\n",
        "\n",
        "  def forward(self,x,t_emb):\n",
        "    residual = x\n",
        "    out = self.norm1(x)\n",
        "    out = nn.SiLU()(out)\n",
        "    out = self.conv1(out)\n",
        "    out = out + self.t_emb_layer(t_emb)[:,:,None,None]\n",
        "    out = self.norm2(out)\n",
        "    out = nn.SiLU()(out)\n",
        "    out = self.conv2(out)\n",
        "    out = out + self.residual_conv(residual)\n",
        "    return out"
      ],
      "metadata": {
        "id": "iuXmeKd7FauZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpSampler(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels,skip_channels, num_heads, t_emb_dim) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.upsample = nn.Upsample(mode=\"bilinear\",scale_factor=2)\n",
        "        self.conv = nn.Conv2d(in_channels=in_channels,out_channels=in_channels,kernel_size=3,padding=1,stride=1)\n",
        "\n",
        "        total_channels = in_channels + skip_channels\n",
        "        self.resnet = ResnetBlock(total_channels, out_channels, t_emb_dim)\n",
        "\n",
        "        self.attention_norm = nn.GroupNorm(8, out_channels)\n",
        "        self.attention = nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n",
        "\n",
        "    def forward(self, x, out_down, t_emb):\n",
        "\n",
        "        x_upsampled = self.upsample(x)\n",
        "        x_upsampled = self.conv(x_upsampled)\n",
        "\n",
        "        x_concat = torch.cat([x_upsampled, out_down], dim=1)\n",
        "\n",
        "        out = self.resnet(x_concat, t_emb)\n",
        "\n",
        "        batch_size, channels, h, w = out.shape\n",
        "        in_attn = out.reshape(batch_size, channels, h * w)\n",
        "        in_attn = self.attention_norm(in_attn)\n",
        "        in_attn = in_attn.transpose(1, 2)\n",
        "        out_attn, _ = self.attention(in_attn, in_attn, in_attn)\n",
        "        out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
        "        out = out_attn + out\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "tYtbtTN3GVyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Unet(nn.Module):\n",
        "    def __init__(self, in_channels=3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.down_channels = [32, 64, 128, 256]\n",
        "        self.up_channels = [256, 128, 64, 32]\n",
        "        self.t_emb_dim = 128\n",
        "        self.down_sample = [True, True, True, False]\n",
        "\n",
        "        self.t_proj = nn.Sequential(\n",
        "            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(self.t_emb_dim, self.t_emb_dim)\n",
        "        )\n",
        "\n",
        "        self.conv_in = nn.Conv2d(in_channels=in_channels, out_channels=self.down_channels[0],\n",
        "                                kernel_size=3, padding=1, stride=1)\n",
        "\n",
        "        self.downs = nn.ModuleList([])\n",
        "        for i in range(len(self.down_channels) - 1):\n",
        "            self.downs.append(\n",
        "                DownBlock(in_channels=self.down_channels[i],\n",
        "                         out_channels=self.down_channels[i + 1],\n",
        "                         num_heads=4,\n",
        "                         t_emb_dim=self.t_emb_dim,\n",
        "                         down_sample=self.down_sample[i])\n",
        "            )\n",
        "\n",
        "        self.mids = nn.ModuleList([\n",
        "            MiddleBlock(in_channels=256, out_channels=256, t_emb_dim=self.t_emb_dim, num_heads=4),\n",
        "            MiddleBlock(in_channels=256, out_channels=256, t_emb_dim=self.t_emb_dim, num_heads=4)\n",
        "        ])\n",
        "\n",
        "        self.ups = nn.ModuleList([])\n",
        "        for i in range(len(self.up_channels) - 1):\n",
        "          skip_index = len(self.up_channels)-2-i\n",
        "          skip_channel = self.down_channels[skip_index]\n",
        "          self.ups.append(\n",
        "              UpSampler(in_channels=self.up_channels[i],\n",
        "                        out_channels=self.up_channels[i + 1],\n",
        "                        skip_channels=skip_channel,\n",
        "                        num_heads=4,\n",
        "                        t_emb_dim=self.t_emb_dim)\n",
        "          )\n",
        "\n",
        "        self.norm_out = nn.GroupNorm(8, 32)\n",
        "        self.conv_out = nn.Conv2d(32, in_channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        out = self.conv_in(x)\n",
        "\n",
        "        t_emb = get_time_emb(t, self.t_emb_dim)\n",
        "        t_emb = t_emb.to(x.dtype)\n",
        "        t_emb = self.t_proj(t_emb)\n",
        "\n",
        "        down_outs = []\n",
        "        for down in self.downs:\n",
        "            down_outs.append(out)\n",
        "            out = down(out, t_emb)\n",
        "\n",
        "        for mid in self.mids:\n",
        "            out = mid(out, t_emb)\n",
        "\n",
        "        for up in self.ups:\n",
        "            down_out = down_outs.pop()\n",
        "            out = up(out, down_out, t_emb)\n",
        "\n",
        "        out = nn.SiLU()(self.norm_out(out))\n",
        "        out = self.conv_out(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "oHh3pHFMJNeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bitsandbytes"
      ],
      "metadata": {
        "id": "lGZRIE4XmNSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "\n",
        "def train_diffusion_model(model, dataloader, noise_scheduler, num_epochs=100, learning_rate=1e-4):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    from bitsandbytes.optim import AdamW8bit\n",
        "    optimizer = AdamW8bit(model.parameters(), lr=learning_rate)\n",
        "    model.train()\n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch_idx, images in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
        "            images = images.half().to(device)\n",
        "            batch_size = images.shape[0]\n",
        "            timesteps = torch.randint(0, noise_scheduler.num_time_steps, (batch_size,), device=device)\n",
        "            noise = torch.randn_like(images)\n",
        "            noisy_images = noise_scheduler.add_noise(images, noise, timesteps)\n",
        "            predicted_noise = model(noisy_images, timesteps)\n",
        "            loss = F.mse_loss(predicted_noise, noise)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f\"Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        avg_loss = epoch_loss / num_batches\n",
        "        losses.append(avg_loss)\n",
        "        print(f\"Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            generated = sample_from_diffusion(\n",
        "                model, noise_scheduler,\n",
        "                img_shape=(3, 64, 64),\n",
        "                num_steps=noise_scheduler.num_time_steps,\n",
        "                device=device\n",
        "            )\n",
        "            img = generated.squeeze().permute(1, 2, 0).cpu().numpy()\n",
        "            plt.figure(figsize=(2, 2))\n",
        "            plt.imshow(img)\n",
        "            plt.axis('off')\n",
        "            plt.title(f\"Generated Sample - Epoch {epoch+1}\")\n",
        "            plt.show()\n",
        "        model.train()\n",
        "    return losses\n",
        "\n",
        "\n",
        "def sample_from_diffusion(model, noise_scheduler, img_shape=(3, 32, 32), num_steps=1000, device=None):\n",
        "    import torch\n",
        "\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.eval()\n",
        "    x = torch.randn(1, *img_shape, device=device, dtype=torch.float16)\n",
        "\n",
        "    for t in tqdm(reversed(range(num_steps)), desc=\"Sampling\"):\n",
        "        time_steps = torch.full((1,), t, device=device, dtype=torch.long)\n",
        "        with torch.no_grad():\n",
        "            pred_noise = model(x, time_steps)\n",
        "\n",
        "        alpha_t = noise_scheduler.alpha[t].to(device, dtype=x.dtype)\n",
        "        alpha_bar_t = noise_scheduler.alpha_bar[t].to(device, dtype=x.dtype)\n",
        "        beta_t = noise_scheduler.beta[t].to(device, dtype=x.dtype)\n",
        "        sqrt_one_minus_alpha_bar_t = noise_scheduler.sqrt_one_minus_alpha_bar[t].to(device, dtype=x.dtype)\n",
        "\n",
        "        coef1 = 1 / torch.sqrt(alpha_t)\n",
        "        coef2 = beta_t / sqrt_one_minus_alpha_bar_t\n",
        "        x_prev_mean = coef1 * (x - coef2 * pred_noise)\n",
        "\n",
        "        if t > 0:\n",
        "            noise = torch.randn_like(x)\n",
        "            var_t = beta_t\n",
        "            x = x_prev_mean + torch.sqrt(var_t) * noise\n",
        "        else:\n",
        "            x = x_prev_mean\n",
        "\n",
        "    x = x.clamp(-1, 1).float()\n",
        "    x = (x + 1) / 2\n",
        "    return x.cpu()\n",
        "\n",
        "\n",
        "\n",
        "def run_training():\n",
        "    model = Unet(in_channels=3).half()\n",
        "    noise_scheduler = NoiseScheduler(betas_start=1e-4, betas_end=0.02, num_time_steps=250)\n",
        "    losses = train_diffusion_model(\n",
        "        model=model,\n",
        "        dataloader=dataloader,\n",
        "        noise_scheduler=noise_scheduler,\n",
        "        num_epochs=70,\n",
        "        learning_rate=1e-4\n",
        "    )\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(losses)\n",
        "    plt.title('Training Loss Over Time')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.show()\n",
        "    generated = sample_from_diffusion(model, noise_scheduler, img_shape=(3, 32, 32), num_steps=1000)\n",
        "    grid_img = generated.squeeze().permute(1, 2, 0).numpy()\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    plt.imshow(grid_img)\n",
        "    plt.axis('off')\n",
        "    plt.title('Generated Emoji Image')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "XpvWcW0WXlS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_training()"
      ],
      "metadata": {
        "id": "_dhbiXWzZocU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}